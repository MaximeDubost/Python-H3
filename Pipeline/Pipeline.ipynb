{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings, sys\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataHandle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandle:\n",
    "\n",
    "    @classmethod\n",
    "    def get_data(self):\n",
    "        \"\"\" \n",
    "        Get data from GCS Bucket \n",
    "        \"\"\"\n",
    "        print('[1/2] Getting data... ', end='')\n",
    "        df1 = pd.read_csv('https://storage.googleapis.com/h3-data/listings_final.csv', sep=';')\n",
    "        df2 = pd.read_csv('https://storage.googleapis.com/h3-data/price_availability.csv', sep=';')\n",
    "        print('Done.')\n",
    "        return [df1, df2]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_group_data(self, data):\n",
    "        \"\"\" \n",
    "        Merge both dataframes' data \n",
    "        \"\"\"\n",
    "        print('[2/2] Merging data... ', end='')\n",
    "        result = pd.merge(data[0], data[1].groupby('listing_id').local_price.mean('local_price'), how='inner', on='listing_id')\n",
    "        print('Done.')\n",
    "        return result\n",
    "    \n",
    "    @classmethod\n",
    "    def get_process_data(self):\n",
    "        \"\"\" \n",
    "        Get & Merge data \n",
    "        \"\"\"\n",
    "        print(\"===| DataHandle |=== \\n\")\n",
    "        result = self.get_group_data(self.get_data())\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===| DataHandle |=== \n",
      "\n",
      "[1/2] Getting data... Done.\n",
      "[2/2] Merging data... Done.\n"
     ]
    }
   ],
   "source": [
    "df = DataHandle.get_process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureRecipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureRecipe:\n",
    "    data = None\n",
    "    variable_types = None\n",
    "    useless_columns = []\n",
    "    thresholded_columns = []\n",
    "    duplicated_columns = []\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        FeatureRecipe's Constructor\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "    \n",
    "        \n",
    "    def separate_variable_types(self):\n",
    "        \"\"\" \n",
    "        Separate column variable types on lists \n",
    "        \"\"\"\n",
    "        print('[1/5] Separate variable types... ', end='')\n",
    "        \n",
    "        discreet, continues, boolean, categorical = [], [], [], []\n",
    "        for column in self.data.columns:\n",
    "            if self.data[column].dtype == np.dtype('int64'):\n",
    "                discreet.append(self.data[column].name)\n",
    "            elif self.data[column].dtype == np.dtype('float64'):\n",
    "                continues.append(self.data[column].name)\n",
    "            elif self.data[column].dtype == np.dtype('bool'):\n",
    "                boolean.append(self.data[column].name)\n",
    "            else:\n",
    "                categorical.append(self.data[column].name)    \n",
    "        self.variable_types = {\"discreet\": discreet, \"continues\": continues, \"boolean\": boolean, \"categorical\": categorical}\n",
    "\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    \n",
    "    def drop_uselessf(self):\n",
    "        \"\"\" \n",
    "        Drop useless columns \n",
    "        \"\"\"\n",
    "        print('[2/5] Dropping useless features... ', end='')\n",
    "\n",
    "        if \"Unnamed: 0\" in self.data.columns:\n",
    "            self.useless_columns.append('Unnamed: 0')\n",
    "        \n",
    "        for column in self.data.columns:\n",
    "            if self.data[column].isna().sum == len(self.data[column]):\n",
    "                self.useless_columns.append(self.data[column].name)\n",
    "\n",
    "        self.data.drop(columns=self.useless_columns, inplace=True)\n",
    "            \n",
    "        print(\"Done.\")\n",
    "        \n",
    "        \n",
    "    def deal_duplicate(self):\n",
    "        \"\"\" \n",
    "        Drop duplicated columns \n",
    "        \"\"\"\n",
    "        print('[3/5] Dropping duplicates... ', end='')\n",
    "        \n",
    "        for col1_i in range(self.data.shape[1]):\n",
    "            for col2_i in range(col1_i+1, self.data.shape[1]):\n",
    "                if self.data.iloc[:, col1_i].equals(self.data.iloc[:, col2_i]):\n",
    "                    self.duplicated_columns.append(self.data.iloc[:, col2_i].name)\n",
    "        \n",
    "        self.data.drop(columns=self.duplicated_columns, inplace=True)\n",
    "        \n",
    "        print(\"Done.\")\n",
    "          \n",
    "    \n",
    "    def drop_nanp(self, thresold: float):\n",
    "        \"\"\" \n",
    "        Drop NaN columns according to a thresold \n",
    "        \n",
    "        @params:\n",
    "            - thresold: value between 0 included and 1 excluded\n",
    "        \"\"\"\n",
    "        print(\"[4/5] Dropping NaN columns according to thresold (\" + str(thresold) + \")... \", end='')\n",
    "        \n",
    "        for column in self.data.columns:\n",
    "            if (self.data[column].isna().sum() / self.data.shape[0]) > thresold:\n",
    "                self.thresholded_columns.append(column)\n",
    "                \n",
    "        self.data.drop(columns=self.thresholded_columns, inplace=True)\n",
    "        \n",
    "        print(\"Done.\")\n",
    "        \n",
    "        \n",
    "    def deal_dtime(self):\n",
    "        \"\"\" TODO : Traiter les DateTime \"\"\"\n",
    "        print('[5/5] Dealing DateTime... Not Implemented')\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def prepare_data(self, threshold: float):\n",
    "        print(\"===| FeatureRecipe |=== \\n\")\n",
    "        feature_recipe.separate_variable_types()\n",
    "        feature_recipe.drop_uselessf()\n",
    "        feature_recipe.deal_duplicate()\n",
    "        feature_recipe.drop_nanp(threshold)\n",
    "        feature_recipe.deal_dtime()\n",
    "        \n",
    "        print(\"\\n• Variable types :\")\n",
    "        for vtype in self.variable_types.keys():\n",
    "            print(\"- \" + str(vtype) + \" : \" + str(self.variable_types[vtype]))\n",
    "        print(\"\\n• Useless dropped columns (\" + str(len(self.useless_columns)) + \" column(s)) : \" + str(self.useless_columns))\n",
    "        print(\"\\n• Duplicated dropped columns (\" + str(len(self.duplicated_columns)) + \" column(s)) : \" + str(self.duplicated_columns))\n",
    "        print(\"\\n• Nan dropped columns by thresold (\" + str(len(self.thresholded_columns)) + \" column(s) for a thresold of \" + str(threshold) + \") : \" + str(self.thresholded_columns))\n",
    "        print(\"\\n• Deal DateTime : Not Implemented\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===| FeatureRecipe |=== \n",
      "\n",
      "[1/5] Separate variable types... Done.\n",
      "[2/5] Dropping useless features... Done.\n",
      "[3/5] Dropping duplicates... Done.\n",
      "[4/5] Dropping NaN columns according to thresold (0.05)... Done.\n",
      "[5/5] Dealing DateTime... Not Implemented\n",
      "\n",
      "• Variable types :\n",
      "- discreet : ['Unnamed: 0', 'listing_id', 'person_capacity', 'beds', 'bedrooms']\n",
      "- continues : ['latitude', 'longitude', 'bathrooms', 'pricing_weekly_factor', 'pricing_monthly_factor', 'local_price']\n",
      "- boolean : ['is_rebookable', 'is_new_listing', 'is_fully_refundable', 'is_host_highly_rated', 'is_business_travel_ready']\n",
      "- categorical : ['name', 'type', 'city', 'neighborhood']\n",
      "\n",
      "• Useless dropped columns (1 column(s)) : ['Unnamed: 0']\n",
      "\n",
      "• Duplicated dropped columns (1 column(s)) : ['is_business_travel_ready']\n",
      "\n",
      "• Nan dropped columns by thresold (1 column(s) for a thresold of 0.05) : ['neighborhood']\n",
      "\n",
      "• Deal DateTime : Not Implemented\n"
     ]
    }
   ],
   "source": [
    "feature_recipe = FeatureRecipe(df)\n",
    "feature_recipe.prepare_data(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, unused_columns:list):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = None, None, None, None\n",
    "        self.data = data\n",
    "        self.unused_columns = unused_columns\n",
    "        \n",
    "    def drop_unusedf(self):\n",
    "        print('[1/2] Dropping unused columns... ', end='')\n",
    "        for column in self.data.columns:\n",
    "            if column in self.unused_columns:\n",
    "                self.data.drop(columns=column, inplace=True)\n",
    "        print('Done.')\n",
    "    \n",
    "    def split_data(self, test_size: float, random_state: int, target: str):\n",
    "        print('[2/2] Splitting data... ', end='')\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.data.loc[:, self.data.columns != target], self.data[target], test_size=test_size, random_state=random_state)\n",
    "        print('Done.')\n",
    "        \n",
    "    def extract(self):\n",
    "        print(\"===| FeatureExtractor |=== \\n\")\n",
    "        self.drop_unusedf()\n",
    "        self.split_data(0.3, 42, 'local_price')\n",
    "        return self.x_train, self.x_test, self.y_train, self.y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===| FeatureExtractor |=== \n",
      "\n",
      "[1/2] Dropping unused columns... Done.\n",
      "[2/2] Splitting data... Done.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(feature_recipe.data, [\"listing_id\", \"name\", \"type\", \"city\", \"neighborhood\", \"latitude\", \"longitude\", \"is_rebookable\", \"is_new_listing\", \"is_fully_refundable\", \"is_host_highly_rated\"])\n",
    "x_train, x_test, y_train, y_test = feature_extractor.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelBuilder (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilder:\n",
    "    \"\"\"\n",
    "        Class for train and print results of ml model \n",
    "    \"\"\"\n",
    "    def __init__(self, model_path: str = None, save: bool = None):\n",
    "        pass\n",
    "    \n",
    "    def __repr__(self):\n",
    "        pass\n",
    "    \n",
    "    def predict_test(self, X) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    def predict_from_dump(self, X) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    def save_model(self, path:str):\n",
    "        #with the format : 'model_{}_{}'.format(date)\n",
    "        pass\n",
    "    \n",
    "    def print_accuracy(self):\n",
    "        pass\n",
    "    \n",
    "    def load_model(self):\n",
    "        try:\n",
    "            #load model\n",
    "            pass\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataManager (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataHandler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b136ee6c60b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mDataHandler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFeatureRecipe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mFeatureExtractor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \"\"\"\n\u001b[0;32m      3\u001b[0m         \u001b[0mFonction\u001b[0m \u001b[0mqui\u001b[0m \u001b[0mlie\u001b[0m \u001b[0mles\u001b[0m \u001b[1;36m3\u001b[0m \u001b[0mpremières\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0mde\u001b[0m \u001b[0mla\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0met\u001b[0m \u001b[0mqui\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mFeatureExtractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataHandler' is not defined"
     ]
    }
   ],
   "source": [
    "def DataManager(d:DataHandler=None, fr: FeatureRecipe=None, fe:FeatureExtractor=None):\n",
    "    \"\"\"\n",
    "        Fonction qui lie les 3 premières classes de la pipeline et qui return FeatureExtractor.split(0.1)\n",
    "    \"\"\"\n",
    "    pass\n",
    "#on appelera la fonction DataManager() de la facon suivante : \n",
    "X_train, X_test, y_train, y_test = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ModelBuilder() \n",
    "m.train(X_train, y_train)\n",
    "m.print_accuracy()\n",
    "m.save_model(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
